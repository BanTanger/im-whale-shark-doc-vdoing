---
title: 消息四大特性
date: 2023-04-05 18:58:40
permalink: /pages/69b7fd/
categories:
  - 项目
  - 高性能IM即时通讯系统
tags:
  - 
author: 
  name: BanTanger | 半糖
  link: https://github.com/bantanger
---
## 背景

只是完成基本消息收发和持久化，还是远远不够的，还要保证消息的四大特性：实时性、有序性、可靠性、幂等性



## 实时性：提高消息处理速率

首先，原始代码无校验发送方合法性情况下接口响应速度 QPS 为：7.8s、28ms、27ms、43ms、25ms（单机）

用 JMeter 模拟 150 线程组的 QPS

![image-20230405230723972](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304052307737.png)



### 线程池优化串行代码

> 线程池只适合流式，不适合放很重的业务逻辑（if-else）只适合按部就班的执行某些步骤

在 IM 系统中，实时性要求很高，每个用户发送的消息都需要及时被处理和发送给接收方，这就需要对线程池的参数进行精细的配置，以确保系统能够满足实时性要求。以下是一些可能的线程池参数配置：

- 核心线程数：CPU 核心数 x 2。这样可以让核心线程数随着 CPU 升级而自动调整，且线程池中的线程数不会过多地占用系统资源，不会拖累整个系统的性能。
- 最大线程数：与核心线程数相同。 IM 系统中可能会存在大量用户对话的情况，对于每个对话线程，需要开辟大量的线程来同时处理各个用户间的消息。此时，最大线程数应该与核心线程数相等，防止因线程数量过多而导致系统性能下降。
- 队列容量：1000。在 IM 系统中，由于每个用户发送消息的时间间隔很短，因此可以快速处理并同步该消息，避免消息被阻塞。将队列容量设置为较小值，可以让消息很快地被处理并发送。
- 空闲线程存活时间：60 秒。 在 IM 系统中，一些长时间没有活动的线程只会占据资源，因此在线程池中定义足够短的空闲线程存活时间非常重要。
- 线程工厂和线程命名：为了分析和排除线程问题，为 IM 系统的每个线程定义工厂和线程名称非常必要，以方便开发和维护人员调试问题和定位相关线程信息。

::: tip 为什么队列容量不易过大

在多线程编程中，任务的执行通常是与线程池相关的队列中的任务竞争，如果队列的容量越大，那么等待线程去处理队列中的任务也就越长。因此，当一个线程池的队列容量设置较小时，可以保证队列中的任务尽快被处理。在实时性要求高的IM系统中，由于每个用户发送消息的间隔短，系统需要尽快地处理和发送消息，如果队列容量太大会导致消息被阻塞，影响系统的实时性。因此，将线程池队列容量设置为较小的值，可以确保接收到的消息及时得到处理和发送，提升系统实时性。

:::

使用后台发送消息接口测试线程池提高系统性能

![image-20230405221741940](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304052217781.png)

线程池

+ 操作人合法：耗时7.86s、42ms、53ms、44ms（单机）

第一次耗时特别长，有次测试甚至达到了恐怖的 16s，主要问题为 TTFB，后端体现是线程池初始化，创建容量为 1000 的任务队列。之后的测试都不超过 55 ms，线程池优化逻辑如此

JMeter 模拟 150 线程组的 QPS：138

![image-20230405231423390](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304052314826.png)

模拟 600 线程组的 QPS：428，请求时间为 1s

![image-20230405231851707](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304052318578.png)

模拟 600 线程组的响应时间，请求时间为 60s

![image-20230406000700530](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304060020878.png)

+ 在前 40 s 时平均响应在 14 ~ 16 ms 波动，之后响应时间缩短

任务队列容量不宜设置太小，按照 gpt 提示设置的 20，下面是平均响应时间

![image-20230406001701702](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304060017119.png)

+ 图中可看到，响应时间会在某一时刻突然飙升，对于实时性要求强的 IM 系统来说，这几乎是不可容忍的，经过测试之后 任务队列容量 1000 比较适合 WhaleShark



### RPC 解耦消息合法性检验

消息的合法性校验应该尽量的早，而不是等到消息进入 MQ 发布者发给监听者后再在业务层处理，无异于增大了 MQ 的带宽，也影响了消息处理的速度（需要查表），因此这里需要将合法性检验解耦，过程提前，让消息的业务逻辑处理尽量呈流式而不是 if - else ，这里我们将合法性检验放在 TCP 层，当校验失败就直接返回 ACK 失败应答报文，就不用将无效消息加入 MQ 了

然而 TCP 层为保证其运行速度和纯粹性也不应该直接与业务层相连，为保证实时性，采用 RPC 进行解耦通讯，技术选型上使用 Feign（HTTP 调用），没有别的理由，只是为了快速开发，后期考虑增加 Dubbo、gRPC 方式，主要是它们太重，集成较为复杂。



![image-20230406102000379](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304061020877.png)



#### 异常：

feignService 获取为空，原因是 feignService 是在 NettyChannelHandler 进行的初始化，我想当然的认为只要注册 Bean 就能使用了，但其实 Feign 不在生命周期里，因此只能使用传参的方式。

![image-20230406102223634](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304061022797.png)

![image-20230406102248416](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304061022415.png)

目前的膨胀接口

~~一直在犹豫一个点，其实并不是所有的策略实现类都需要 FeignService 的，也就是它冗余了，假设我以后的策略实现类越来越多，冗余字段越来越多，感觉有些难以维护，但又不知道从什么地方去优化这个点~~

**1.可以使用 DTO 包装参数，接口只需传入 DTO 即可**，~~要否掉这种情况，因为每次执行策略时都会额外的初始化其他不需要的类，由于不是传参而是实际成员变量，导致每次都需要重新初始化 map。这样有可能会导致内存不足的现象~~，之前是想通过抽象类来将所有需要的参数解析，然后所有子类调用抽象类的成员即可了，但是实现起来有困难。**最终还是在每个策略里解析 dto 具体参数，根据需要来解析**

详细探讨情况如下：

::: details

![image-20230406165525187](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304061655943.png)

+ 使用抽象类来实例化所有的参数，并通过子类调用抽象类构造器来完成实例化。

属于是经验不足了，策略模式中的子类不建议带有参构造，仔细看这段代码，每次进行决策时都会调度 map 初始化逻辑，但每次决策只会选出一个策略实例执行接口逻辑，剩余的策略实例其实根本没被用到。这样做的一个坏处就是会导致策略实例每次都会重新创建占用内存，导致内存占用越来越大。

那只有第一次初始化 map，做一个判空可以吗。其实是不行的，必须每次执行策略时都调用 init，不然 map 里存储的只有第一次进入决策的实体了。（这里我已经把代码改过来了，没法展示之前错误的例子，之后有时间了再来补充一下吧）

:::



~~**2.也可以用可变参数来不断扩展**~~，不太适合这个策略模式的业务



### MQ 异步处理消息持久化

在之前的逻辑中，消息需要先落库持久化才能正常发放消息，但其实落库的过程是最耗时的。在这一步里，我们可以使用 MQ 异步持久化，当消息异步持久化失败之后只需要重试即可，即消息处理过程尽量考虑实时性，舍弃部分持久化

观察之前的持久化代码

![image-20230406082555513](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304060825596.png) 

发现只有持久化时才使用到 ImMessageBodyEntity 实体类，如果我们要使用 MQ 异步持久化，这个实体类可以不用转换。并且之前的设计有些许不合理，持久化失败应该异步重试，而不应该影响消息的实时性，不能因为不能持久化就阻断本次消息。

通过 messageKey 来实现 MQ 异步持久化



![image-20230406172035789](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304061720107.png)

+ MQ 异步持久化成功



## 可靠性

### TCP 可靠性

提到可靠性，我们需要从消息数据传输开始谈起

::: tip 深入到操作系统内核，谈谈用户发送一条消息，对方是怎么接收到的

当用户发送一条消息时，数据首先会进入应用层，应用程序通常会将数据提交到操作系统内核中的套接字，以便内核可以处理网络传输相关的工作。

在内核中，首先，数据需要通过传输层协议（如TCP或UDP）封装为数据包，并添加对应的传输层协议头和尾。这个过程会涉及到数据包的封装、序列号的处理、重发请求的管理等。

接下来，内核会根据目标地址和路由表等信息，将数据包传递到正确的网络接口，例如Ethernet接口或者无线网络接口。这个过程中还经过了IP层（Internet协议层）的处理，包括数据包的封装和解封装等。

最后，数据包会通过物理层被传输给网络中的目的设备，并在目的设备上被逆向处理，直到消息被网络协议栈中的接收端应用程序读取。

在整个过程中，操作系统内核负责管理、维护和处理数据包的传输，包括数据的拆分、重组、重传、错误检测等。

:::

下面是一个双方通信简化的消息数据流转的流程图

![消息传输流转图](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304062102419.png)

操作系统通常都携带网络协议栈，其中最著名的就是 TCP 协议

1. 前端传输的 JSON 数据对象经过后端私有协议处理，转化成二进制流传输到操作系统内核的发送缓冲区中
2. 数据包经过传输层也就是 TCP 层封装一个 TCP 帧头
3. 数据包经过网络层再添加一个 IP 帧头
4. 数据包再经过数据链路层添加一个 MAC 帧头定位唯一设备
5. 最后数据包被操作系统内核发送到物理实际网卡上，网卡发送的数据包通过 DNS 解析和路由层定义等等一系列流程寻找到目标地址



### 有了 TCP 保证可靠性，为什么还要自己保证消息可靠性？

1. 接收缓冲区将数据包发送给应用层的过程中，可能会存在目标用户 APP 闪退，导致数据包无法发送

![image-20230406220947437](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304062209691.png)

2. 异步持久化时容易出现用户闪退、杀后台导致消息丢失

**TCP协议只能保证传输层的可靠，而应用层可靠需要用户自己实现**



### 应用层可靠性保证

![image-20230407224118279](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304072241582.png)

![image-20230406224212353](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304062242458.png)

![image-20230406224150048](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304062241251.png)



接受确认 ACK 数据包结构定义

```java
@Data
public class MessageReceiveAckPack extends ClientInfo {

    /** 消息唯一标识 */
    private Long messageKey;

    private String fromId;

    private String toId;

    private Long messageSequence;

    /** 是否为服务端发送的消息 */
    private boolean serverSend = false;

}
```



### 测试

在线用户向在线用户发送消息

![image-20230407092730935](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304070927930.png)

![image-20230407092520674](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304070925843.png)

+ 目标用户发送接收确认 ACK，sendServer = false



在线用户向离线用户发送消息

![image-20230407092911945](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304070929308.png)

![image-20230407092123553](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304070921821.png)

+ 服务端代替离线用户发送接收确认 ACK，sendServer = true



## 有序性

使用多线程并发进行消息处理固然能提升处理速度，提高消息实时性，但却牺牲了消息的顺序性。基于这种情况，我们需要选择一个递增的字符串值作为消息排序的标杆

这个方案可以是客户端发送消息的生成时间 createTime；也可以是使用 messageKey 雪花算法生成的趋势递增的消息唯一标识；还可以是使用 redis 原子性操作语句 incr 不断递增

依靠客户端机器时钟不太安全，无可避免的是用户可以自由的修改时间，当然服务端可以做一些手段来防止，例如每隔一段时间就同步服务端和客户端的机器时钟，但这样也有局限，会存在一定的消息乱序

雪花算法在一定程度上能保证消息的递增性，但在极端情况下出现机器时钟回拨的现象也会导致消息乱序，除非使用美团的 leaf 改版雪花算法

基于上述两个方案的优劣缺点，决定采用 redis 来递增 Seq



### redis 键值设计

保证每一个 P2P 、群组都有一个绝对增长的 seq，就要设计一个 redis key 来分别存储

```
key: appId + ":message:"/":GroupMessage:" + (fromId_toId) 小在前 / groupId 
```



### 改进（线上bug）

外提 Seq 生成逻辑，因为 seq 生成策略可以是 redis，也可以是一个新的服务专门处理。为了保证安全性需要对第三方接口进行异常捕获，因此不要将这段逻辑脏污线程池的逻辑，保证线程池的流式纯粹



## 幂等性

消息可靠性的实现需要发送方收到两次应答 ACK，一个是服务端接收发送方消息的确认 ACK，一个是目标对象接受到发送方消息的确认 ACK，有了这两个 ACK 就能保证消息的可靠性传输，但弊端也很明显——出现消息重复现象。

思考这样一个场景：发送方 A 发送一条消息给在线用户 B，已经成功接受到服务端的确认 ACK，等待接收方的确认 ACK，但在经过了几次重试之后依然没有接受到，A 认为 B没有接受到数据。但其实 B 接受到数据，并已经异步持久化了，只是由于网络波动没有及时在规定时间内返回确认 ACK，A 再次发送消息之后 B 的确认 ACK 才到达，这样就导致了消息重复的现象

**要保证消息的可靠性，就无法完全的避免消息幂等性的状况**，但可以通过一些简单的手段来遏制，比如说防重 id 来防止同一份数据处理多次，或者是消息达到时做一个缓存，缓存时间尽量短，缓存时间内的消息重试直接让接收方接收消息，不进行二次持久化。当超过缓存时间时从服务端重新更新消息唯一id，messageKey，也就是重新做一个消息体

![image-20230407213023768](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304072130551.png)

![image-20230407213032013](https://cdn.statically.io/gh/BanTanger/image-hosting@master/70.%E6%B6%88%E6%81%AF%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7-assets/202304072130924.png)

key：appId + ":cacheMessage:" + messageId

过期时间: 5 分钟
