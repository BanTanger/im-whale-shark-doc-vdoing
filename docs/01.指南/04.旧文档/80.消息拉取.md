---
title: 消息拉取
date: 2023-04-09 10:48:50
permalink: /pages/d5dc7e/
categories:
  - 项目
  - 高性能IM即时通讯系统
tags:
  - 
author: 
  name: BanTanger | 半糖
  link: https://github.com/bantanger
---
## 背景

其实服务端的逻辑对用户无感，我们在服务端做到的优化手段，可能在客户端眼里没什么关系。而在客户端中最影响用户体验的，是用户第一次登录时拉取好友数据的过程。在之前的架构中，我们拉取数据是全量拉取，并没有在客户端做缓存

也就是用户每次登录，不管对应接口的数据有没有变化，都通通从服务端里拉取并返回给前端进行处理。这显然是非常不友好的。

而每次用户聊天服务端都有复杂的查询操作，这样会对服务端造成极大的压力和浪费

本章节就来修改这一段逻辑，最终需求为只会向服务端请求未同步数据，不涉及其他业务逻辑过滤，减少服务端压力，优化用户体验



## 市场调研

### 优化一方案：延时拉取、按需拉取

本质上还是全量拉取，对于拉取时间上并没有缩短，只是分摊了每次拉取所需要用到的时间

![image-20230409135641645](https://cdn.statically.io/gh/BanTanger/image-hosting@master/80.%E6%B6%88%E6%81%AF%E6%8B%89%E5%8F%96-assets/202304091356947.png)



### 优化二方案：更改数据库表设计

增加数据序列号字段（版本号），用于对比服务端与客户端之间的数据差值，只有当服务端的 Seq 大于 客户端的 Seq 才执行拉取操作，这是一个增量拉取，有效的缩短了拉取耗时，减少无意义的拉取次数

修改之前的数据表设计：`X（id，uid，其他数据字段）`

修改之后的数据表设计：`X（id，uid，其他数据字段，seq）`



假设现在客户端所记录的 Seq 最大数据偏序为 10，服务端 Seq 为 20，最终客户端应该增量拉取 `20 - 10 = 10` 也就是 11 ~ 20 的数据量

对应 sql 语句变化

修改之前：`select 字段 from X where uid = A`

修改之后：`select 字段 from X where uid = A and seq > 10`

![image-20230409140105161](https://cdn.statically.io/gh/BanTanger/image-hosting@master/80.%E6%B6%88%E6%81%AF%E6%8B%89%E5%8F%96-assets/202304091401242.png)

但实际上，经过市场调研和数据量勘察发现，用户的数据量 Seq 在 90% 上和服务端是一致的。而方案二只是优化了拉取的时间，但并没有优化调用拉取接口的次数。

假设现在一个用户有一百个会话窗口，但只有其中三个会话窗口出现了服务端和客户端缓存的 Seq 数据偏序不一致的情况，在方案二中，情况为，用户调用服务端数据同步接口一百次，从服务端拉取到的数据与客户端本地缓存一一做对比。其中 97 次为无意义拉取，只有三次为有意义拉取，更新到客户端缓存中，客户端在执行 UI 渲染



### 优化三方案：服务端与客户端对比表

在之前设计的表结构：`X（id，uid，其他数据字段，seq）`，只在服务端进行存储。

现在的设计中，表结构存储于服务端、客户端双端。

当发送消息拉取申请时，服务端会先拉取客户端本地记录数据，与自身数据一一做差值，出现差值的数据才进行数据同步拉取，这样做大大的减少了无意义的消息拉取以及接口调用，同时也提升了客户端处理数据的能力，毕竟不只是 IM 面向的从来不是单线程，而是多线程并发处理

服务端、客户端表结构 seq 图（待贴）

![image-20230409140037027](https://cdn.statically.io/gh/BanTanger/image-hosting@master/80.%E6%B6%88%E6%81%AF%E6%8B%89%E5%8F%96-assets/202304091400828.png)

（架构图绘画有误，没有单聊消息和群聊消息，他们属于离线消息的范畴）

+ 如果本地数据库 Seq 偏序字段和服务端有差异，则客户端调用各个服务的同步接口（好友申请、数量变更、被拉入群通知）进行增量拉取，再调用离线消息同步接口
+ 如果本地数据库 Seq 偏序字段和服务端没有任何差异，则客户端调用离线消息同步接口进行离线消息同步。



## 消息存储

使用 Redis Hash 存储所有需要拉取的模块信息：

数据结构为：

```java
Key(appId + SeqConstants) : (SeqConstants, incr)
```

之后的拉取只需要客户端本地拉取 redis 里该用户对应 key 的 value：max_seq 值与本地 max_seq 做差值即可得知应当拉取多少条数据



特殊：群组消息的拉取

由于群组是一对多的关系（调度同步拉取接口也是多个），不能使用 redis 来存储偏序字段，而是直接存入数据库表。并且由于用户添加群在设计中是批量的（list），因此在拉取时，从数据库中将用户添加的所有群与客户端所记录的 max_seq 做差值即可得知用户应当拉取多少个群的会话数据，再调用 `server_max_seq - client_max_seq` 个拉取同步接口即可



## 离线用户和在线用户的消息拉取

上述三种优化方案就是离线用户拉取消息的方式，通过从用户离线消息队列拉取数据（限制1024条）

而在线用户拉取数据是直接通过 TCP 通知调度的，在老东家的设计架构中，是通过 HTTP 短连接进行回调，容易出现消息丢失现象，原因是 HTTP 不稳定，且应用层没做好握手协议

拉取数据既可以是 seq 字段，也可以是最后更新时间或者是主键，根据自身业务进行合适选择



### 离线消息拉取

sql 设计，由于现在 app 很多都不是设计成分页获取数据，而是下拉获取数据，对应的后端逻辑也要发生相应的变动，设计传给前端的 vo 中字段为：{maxSeq，isComplated，dataList}

架构设计图

![image-20230409163241533](https://cdn.statically.io/gh/BanTanger/image-hosting@master/80.%E6%B6%88%E6%81%AF%E6%8B%89%E5%8F%96-assets/202304091632242.png)

流程讲解

![image-20230409164830361](https://cdn.statically.io/gh/BanTanger/image-hosting@master/80.%E6%B6%88%E6%81%AF%E6%8B%89%E5%8F%96-assets/202304091648456.png)

1. 客户端用户登录，向服务端进行一次全量拉取。发送 dto，此时客户端的 client_max_seq 是 0
2. 服务端接受到客户端发来的请求体 ,服务端根据 userId 查询 DB，limit 值由产品所调研定制。回送 vo 给客户端，vo: maxSeq = limit * 1
   + 如果这次查询没有查询完该用户的离线数据，就将 isCompleted 设置成 false
   + 反之为 true
3. 客户端接受到 vo 后更新本地 Seq：client_max_seq = max(dataList.seq)；client_max_seq = 50，并根据 isCompleted 决定是否继续向服务端请求拉取。
4. 循环 1， 2， 3 直到 isCompleted = true



每个用户需要向服务端拉取的消息偏序

redis Hash { appId:seq:userId, sequence }

存储所有待拉取的接口的消息偏序：

封装成方法：

```java
/**
 * 记录用户所有模块: 好友、群组、会话的数据偏序
 * Redis Hash 记录
 * uid 做 key, 具体 seq 做 value
 * @param appId
 * @param userId
 * @param type
 * @param seq
 */
public void writeUserSeq(Integer appId, String userId, String type, Long seq) {
    String key = appId + Constants.RedisConstants.SeqPrefix + userId;
    redisTemplate.opsForHash().put(key, type, seq);
}
```

在所有待拉取的接口方法中新增这个方法



![image-20230414205444392](https://cdn.statically.io/gh/BanTanger/image-hosting@master/80.%E6%B6%88%E6%81%AF%E6%8B%89%E5%8F%96-assets/202304142055829.png)

